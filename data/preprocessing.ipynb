{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(imported_dataset):\n",
    "    return imported_dataset['emotion'].value_counts().reset_index(name = 'count').rename(columns={'index':'emotion'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis(dataset, plt_title = 'Label Distribution'):\n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(14,5), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['emotion'], dataset['count'], width=0.6)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=10, labelpad=11) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=12, labelrotation=0)\n",
    "    labels = dataset['emotion']\n",
    "    ax.set_xticks(dataset['emotion'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['emotion'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['emotion'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['emotion'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis_large(dataset, plt_title = 'Label Distribution'):\n",
    "    \n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(25,3), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['emotion'], dataset['count'], width=0.2)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=8, labelpad=12) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=8, labelrotation=0)\n",
    "    labels = dataset['emotion']\n",
    "    ax.set_xticks(dataset['emotion'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['emotion'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['emotion'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['emotion'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { 'ang': \"anger\", 'angry': \"anger\", 'annoyed': \"anger\", 'furious': \"anger\", 'fru': \"anger\", 'annoyance': 'anger', \"disapproval\":'anger',\n",
    "            'exc': \"joy\", 'joyful': \"joy\", 'happiness': \"joy\", 'hap': \"joy\", 'grateful': \"joy\", 'impressed': \"joy\", 'content': \"joy\", 'fun': \"joy\", 'enthusiasm': \"joy\",\n",
    "            'excited': \"joy\", 'excitement': 'joy', \"pride\": 'joy', 'gratitude':'joy',\"approval\": 'joy', 'admiration':'joy','proud': 'joy',\n",
    "            'fea': \"fear\", 'terrified': \"fear\", 'afraid': \"fear\", \n",
    "            'disgusted': 'disgust', 'hate': 'disgust', 'boredom': 'disgust', \n",
    "            'neu': \"neutral\", \n",
    "            'sad': \"sadness\", 'devastated': \"sadness\", 'disappointed': \"sadness\", \"grief\": \"sadness\", \"lonely\":'sadness','disappointment':'sadness',\n",
    "            'sur': \"surprise\",'surprised': \"surprise\",'sup': \"surprise\", 'realization':'surprise',\n",
    "            'hope': \"optimism\", 'faithful': \"optimism\", 'hopeful': 'optimism', 'confident':'optimism', 'prepared':'optimism',\n",
    "            'guilty': \"guilt\", 'shame': \"guilt\", 'ashamed': \"guilt\", 'embarrassed': \"guilt\",\n",
    "            'caring': \"love\",\n",
    "            'anxious': \"anxiety\", 'worry': \"anxiety\", 'apprehensive': \"anxiety\",'nervousness':'anxiety',\n",
    "            'anticipating': 'anticipation',\n",
    "            'amusement': 'amusement',\n",
    "            'neu': 'neutral',\n",
    "            'confusion':'curiosity'\n",
    "            }\n",
    "\n",
    "class MyDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return key\n",
    "    \n",
    "def map_and_concat(df,list_of_datasets,list_of_ds_name):\n",
    "    total_rows = 0\n",
    "    for d,name in zip(list_of_datasets,list_of_ds_name):\n",
    "        emo_labels = d['label']\n",
    "        d['label'] = emo_labels.map(MyDict(mapping))\n",
    "        ds = d[['sentence', 'label']]\n",
    "        filtered_ds = ds[~ds['label'].str.contains('trust', case=False)]\n",
    "        filtered_ds['ds_name'] = name\n",
    "        df = pd.concat([df,filtered_ds])\n",
    "        total_rows = total_rows + ds.shape[0]\n",
    "        print(total_rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "val_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "test_final = pd.DataFrame(columns=['sentence','label','ds_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needs to be lower letter\n",
    "def relabel_emotion(emotion):\n",
    "    if isinstance(emotion, int):\n",
    "        # for daily dialouge\n",
    "        if emotion in [0] : return 'neutral'\n",
    "        if emotion in [1] : return 'anger'\n",
    "        if emotion in [2] : return 'disgust'\n",
    "        if emotion in [3] : return 'fear'\n",
    "        if emotion in [4] : return 'joy'\n",
    "        if emotion in [5] : return 'sadness'\n",
    "        if emotion in [6] : return 'surprise'\n",
    "    else:\n",
    "        emotion = emotion.lower()\n",
    "        if emotion in ['mad','angry', 1, 'anger'] : return 'anger'\n",
    "        if emotion in ['fear','scared', 3] : return 'fear'\n",
    "        if emotion in ['joy','happy','joyful','happiness', 4] : return 'joy'\n",
    "        if emotion in ['sadness','sad', 5] : return 'sadness'\n",
    "        else : return emotion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Import, Cleaning, Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoEmotion (hartmann)\n",
    "https://github.com/google-research/google-research/tree/master/goemotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_go_emo(dataset):\n",
    "    dataset = dataset.dropna(subset=['emotion_label'])\n",
    "    for idx, row in dataset.iterrows():\n",
    "        emotion_label_txt = row['emotion_label']\n",
    "        emotion_label_list = [int(num) for num in emotion_label_txt.split(\",\")]\n",
    "        row['emotion_label'] = emotion_label_list\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_mapping = {\n",
    "  '0': 'admiration',\n",
    "  '1': 'amusement',\n",
    "  '2': 'anger',\n",
    "  '3': 'annoyance',\n",
    "  '4': 'approval',\n",
    "  '5': 'caring',\n",
    "  '6': 'confusion',\n",
    "  '7': 'curiosity',\n",
    "  '8': 'desire',\n",
    "  '9': 'disappointment',\n",
    "  '10': 'disapproval',\n",
    "  '11': 'disgust',\n",
    "  '12': 'embarrassment',\n",
    "  '13': 'excitement',\n",
    "  '14': 'fear',\n",
    "  '15': 'gratitude',\n",
    "  '16': 'grief',\n",
    "  '17': 'joy',\n",
    "  '18': 'love',\n",
    "  '19': 'nervousness',\n",
    "  '20': 'optimism',\n",
    "  '21': 'pride',\n",
    "  '22': 'realization',\n",
    "  '23': 'relief',\n",
    "  '24': 'remorse',\n",
    "  '25': 'sadness',\n",
    "  '26': 'surprise',\n",
    "  '27': 'neutral'\n",
    "}\n",
    "\n",
    "def map_haru_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        if 1 in e:\n",
    "            final_result_emo.append(\"amusement\")\n",
    "        elif 6 in e or 7 in e:\n",
    "            final_result_emo.append(\"curiosity\")\n",
    "        elif 12 in e or 24 in e:\n",
    "            final_result_emo.append(\"guilt\")\n",
    "        elif 20 in e:\n",
    "            final_result_emo.append(\"optimism\")\n",
    "        elif 5 in e or 18 in e:\n",
    "            final_result_emo.append(\"love\")    \n",
    "        else:\n",
    "            l = e[random.randint(0,len(e)-1)]\n",
    "            final_result_emo.append(orig_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_fine = map_haru_emotion(format_go_emo(train)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_fine = map_haru_emotion(format_go_emo(test)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_fine = map_haru_emotion(format_go_emo(val)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>rater_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m really sorry about your situation :( Altho...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>eecwqtt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's wonderful because it's awful. At not with.</td>\n",
       "      <td>admiration</td>\n",
       "      <td>ed5f85d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kings fan here, good luck to you guys! Will be...</td>\n",
       "      <td>excitement</td>\n",
       "      <td>een27c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't know that, thank you for teaching me ...</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>eelgwd1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They got bored from haunting earth for thousan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>eem5uti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>Thanks. I was diagnosed with BP 1 after the ho...</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>efeeasc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>Well that makes sense.</td>\n",
       "      <td>approval</td>\n",
       "      <td>ef9c7s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>Daddy issues [NAME]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>efbiugo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>So glad I discovered that subreddit a couple m...</td>\n",
       "      <td>admiration</td>\n",
       "      <td>efbvgp9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>Had to watch \"Elmo in Grouchland\" one time too...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>edtjpv6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5427 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence      labels rater_id\n",
       "0     I’m really sorry about your situation :( Altho...     sadness  eecwqtt\n",
       "1       It's wonderful because it's awful. At not with.  admiration  ed5f85d\n",
       "2     Kings fan here, good luck to you guys! Will be...  excitement  een27c3\n",
       "3     I didn't know that, thank you for teaching me ...   gratitude  eelgwd1\n",
       "4     They got bored from haunting earth for thousan...     neutral  eem5uti\n",
       "...                                                 ...         ...      ...\n",
       "5422  Thanks. I was diagnosed with BP 1 after the ho...   gratitude  efeeasc\n",
       "5423                             Well that makes sense.    approval  ef9c7s3\n",
       "5424                                Daddy issues [NAME]     neutral  efbiugo\n",
       "5425  So glad I discovered that subreddit a couple m...  admiration  efbvgp9\n",
       "5426  Had to watch \"Elmo in Grouchland\" one time too...     neutral  edtjpv6\n",
       "\n",
       "[5427 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_fine_goemo.csv\")\n",
    "test_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_fine_goemo.csv\")\n",
    "val_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_fine_goemo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ekman (mapped all fine-grained to Ekman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekman_mapping = {\n",
    "\"0\" : \"anger\",\n",
    "\"1\" : \"disgust\",\n",
    "\"2\" : \"fear\",\n",
    "\"3\" : \"joy\",\n",
    "\"4\" : \"neutral\",\n",
    "\"5\" : \"sadness\",\n",
    "\"6\" : \"surprise\"\n",
    "}\n",
    "def map_ekman_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        l = e[random.randint(0,len(e)-1)]\n",
    "        final_result_emo.append(ekman_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_ekman = map_ekman_emotion(format_go_emo(train_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_ekman = map_ekman_emotion(format_go_emo(test_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_ekman = map_ekman_emotion(format_go_emo(val_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_fine_goemo.csv\")\n",
    "test_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_fine_goemo.csv\")\n",
    "val_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_fine_goemo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43410\n",
      "5426\n",
      "5427\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train_ekman],['goemotion'])\n",
    "val_final = map_and_concat(val_final,[val_ekman],['goemotion'])\n",
    "test_final = map_and_concat(test_final,[test_ekman],['goemotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmoryNLP (BM)\n",
    "[link] https://github.com/emorynlp/emotion-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-cecdd3f673368c60/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b40fdedc954163bd0149297f24952f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-53f09f25d41a1e93/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dee85755be24432b2597342d083adf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-c3a8fb9e451079f1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee80fad5f6f486ca2c3d8eff0b93081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emonlp_train = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-trn.json\" }, field=\"episodes\")\n",
    "emonlp_val = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-dev.json\" }, field=\"episodes\")\n",
    "emonlp_test = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-tst.json\" }, field=\"episodes\")\n",
    "\n",
    "\n",
    "def format_emonlp(emonlp_dataset):\n",
    "    utterances_emonlp = []\n",
    "    emotions_emonlp = []\n",
    "    for row in emonlp_dataset['train']:\n",
    "        scenes = row['scenes']\n",
    "        for scene in scenes:\n",
    "            utterances = scene['utterances']\n",
    "            for utterance in utterances:\n",
    "                # extract only text and emotion labels\n",
    "                utterances_emonlp.append(utterance['transcript'])\n",
    "                emotions_emonlp.append(relabel_emotion(utterance['emotion']))\n",
    "    clean_emonlp = {}\n",
    "    clean_emonlp[\"sentence\"] = utterances_emonlp\n",
    "    clean_emonlp[\"labels\"] = emotions_emonlp\n",
    "    clean_emonlp = Dataset.from_dict(clean_emonlp).filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_emonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b98d4869ca4872bb2c8b0e0839045a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38258b47043a4378b28c38d9a305012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2f2b7493a54fb597f1ba2687197dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_emonlp = format_emonlp(emonlp_train).to_pandas()\n",
    "val_emonlp = format_emonlp(emonlp_val).to_pandas()\n",
    "test_emonlp = format_emonlp(emonlp_test).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emonlp.to_csv('dataset/Emorynlp/emonlp_train_clean.csv', index = False)\n",
    "val_emonlp.to_csv('dataset/Emorynlp/emonlp_val_clean.csv', index = False)\n",
    "test_emonlp.to_csv('dataset/Emorynlp/emonlp_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9934\n",
      "1344\n",
      "1328\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train_emonlp],[\"emoryNLP\"])\n",
    "val_final = map_and_concat(val_final,[val_emonlp],[\"emoryNLP\"])\n",
    "test_final = map_and_concat(test_final,[test_emonlp],[\"emoryNLP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empathetic Dialogues (excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DailyDialog (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = load_dataset(\"daily_dialog\")\n",
    "\n",
    "def format_dd(daily):\n",
    "    utterances_daily = []\n",
    "    for d in daily[\"dialog\"]:\n",
    "        utterances_daily = utterances_daily + d\n",
    "    emotions_daily = []\n",
    "    for d in daily[\"emotion\"]:\n",
    "        emotions_daily = emotions_daily + d\n",
    "    clean_daily = {}\n",
    "    clean_daily[\"utterance\"] = utterances_daily\n",
    "    clean_daily[\"emotion\"] = []\n",
    "    for emo in emotions_daily : clean_daily[\"emotion\"].append(relabel_emotion(emo))\n",
    "    clean_daily = Dataset.from_dict(clean_daily)\n",
    "    clean_daily = clean_daily.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_daily\n",
    "\n",
    "train_dd = format_dd(daily['train']).to_pandas()\n",
    "val_dd = format_dd(daily['validation']).to_pandas()\n",
    "test_dd = format_dd(daily['test']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train_dd],[\"daily\"])\n",
    "val_final = map_and_concat(val_final,[val_dd],[\"daily\"])\n",
    "test_final = map_and_concat(test_final,[test_dd],[\"daily\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEMOCAP (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_utterance = []\n",
    "def split_string_u(string):\n",
    "    pattern = r'^(.*?)(\\[[\\d.-]+\\]:\\s*)(.*)$'\n",
    "    match = re.match(pattern, string)\n",
    "    \n",
    "    if match:\n",
    "        part1 = match.group(1).rstrip()  # Remove trailing spaces from the first part\n",
    "        part2 = match.group(2).rstrip().rstrip(\":\")  # Remove trailing spaces from the second part\n",
    "        part3 = match.group(3)\n",
    "        return part1, part2, part3\n",
    "    else:\n",
    "        excluded_utterance.append(string)\n",
    "\n",
    "def split_string_e(string):\n",
    "   return string.strip().split('\\t')\n",
    "def read_transcription(path):\n",
    "    transcript_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    utterance_list = []\n",
    "    for u in transcript_ds['test']['text']:\n",
    "        utterance = split_string_u(u)\n",
    "        utterance_list.append(utterance)\n",
    "    return utterance_list\n",
    "\n",
    "def read_emotion(path):\n",
    "    emo_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    print(path + '\\n')\n",
    "    emotion_list = []\n",
    "    for i,e in enumerate(emo_ds['test']['text']):\n",
    "        if (e.strip() == '') and (i + 1 < len(emo_ds['test']['text'])):\n",
    "            emotion = split_string_e(emo_ds['test']['text'][i+1])\n",
    "            emotion_list.append(emotion)        \n",
    "    return emotion_list\n",
    "\n",
    "def read_iemocap(dir_path):\n",
    "    # initial two result df to store all transcriptions from all sessions\n",
    "    # similarly for emotions\n",
    "    transcriptions_df = pd.DataFrame(columns=['idx', 'labs', 'utterance', 'session'])\n",
    "    emotions_df = pd.DataFrame(columns=['labs', 'idx', 'emotion', 'attribute', 'session'])\n",
    "    sessions = ['Session1','Session2','Session3','Session4', 'Session5']\n",
    "    for session in sessions:\n",
    "        # get all transcriptions\n",
    "        transcriptions = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/transcriptions'\n",
    "        all_utterances = []\n",
    "        for item in os.listdir(transcriptions):\n",
    "            item_path = os.path.join(transcriptions, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_utterances = all_utterances + read_transcription(item_path)\n",
    "        u_df = pd.DataFrame(all_utterances, columns=['idx', 'labs', 'utterance'])\n",
    "        u_df['session'] = session\n",
    "        transcriptions_df = pd.concat([transcriptions_df, u_df])\n",
    "        \n",
    "        # get all emotions\n",
    "        emoeval = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/EmoEvaluation'\n",
    "        all_emotions = []\n",
    "        for item in os.listdir(emoeval):\n",
    "            item_path = os.path.join(emoeval, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_emotions = all_emotions + read_emotion(item_path)\n",
    "        e_df = pd.DataFrame(all_emotions, columns=['labs', 'idx', 'emotion', 'attribute']) \n",
    "        e_df['session'] = session\n",
    "        emotions_df = pd.concat([emotions_df, e_df])\n",
    "\n",
    "    return {'utterance':transcriptions_df, \n",
    "            'emotion': emotions_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iemocap_dir = 'dataset/IEMOCAP_full_release_withoutVideos'\n",
    "# join by idx\n",
    "read_results = read_iemocap(iemocap_dir)\n",
    "clean_imocap = pd.merge(read_results['utterance'],read_results['emotion'], on='idx')\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'xxx']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'dis']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'oth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test val split: stratify base on labels\n",
    "train, testval= train_test_split(clean_imocap, test_size=0.20, random_state=0, stratify=clean_imocap[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n",
    "\n",
    "train.to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_train.csv')\n",
    "test.to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_test.csv')\n",
    "val.to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train],[\"iemocap\"])\n",
    "val_final = map_and_concat(val_final,[val],[\"iemocap\"])\n",
    "test_final = map_and_concat(test_final,[test],[\"iemocap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MELD (BM) (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld = load_dataset(\"csv\", data_files={\"test\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/test_sent_emo.csv\",\n",
    "                                       \"train\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/train_sent_emo.csv\",\n",
    "                                       \"val\": \"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/dev_sent_emo.csv\"})\n",
    "\n",
    "def format_save_meld(meld):\n",
    "    clean_meld = {}\n",
    "    clean_meld[\"utterance\"] = meld[\"Utterance\"]\n",
    "\n",
    "    clean_meld[\"emotion\"] = []\n",
    "    for emo in meld[\"Emotion\"] : clean_meld[\"emotion\"].append(relabel_emotion(emo))\n",
    "    clean_meld = Dataset.from_dict(clean_meld)\n",
    "    clean_meld = clean_meld.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train = format_save_meld(meld['train'])\n",
    "meld_val = format_save_meld(meld['val'])\n",
    "meld_test = format_save_meld(meld['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train.to_csv('dataset/meld_train_clean.csv', index = False)\n",
    "meld_val.to_csv('dataset/meld_val_clean.csv', index = False)\n",
    "meld_test.to_csv('dataset/meld_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemEval-2018, EI-reg, Mohammad et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_semeval(filename, dataset):\n",
    "    new_df = pd.DataFrame(columns=['ID', 'utterance', 'emotion'],dtype=object)\n",
    "    for index, row in dataset.iterrows():\n",
    "        utterance = row['Tweet']\n",
    "        emotions_col = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "        id = index\n",
    "        emotions_list = []\n",
    "        for emotion in emotions_col:\n",
    "            if row[emotion] == 1:\n",
    "                emotions_list.append(emotion) \n",
    "        \n",
    "        if emotions_list: \n",
    "            e = emotions_list[random.randint(0,len(emotions_list)-1)]\n",
    "            new_row = {\"ID\": id,\n",
    "                    \"utterance\": utterance,\n",
    "                    \"emotion\": e}\n",
    "            print(new_row)\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    new_df.to_csv(\"dataset/SemEval-2018/\" + filename)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n",
    "prep_semeval('semeval_train_clean.csv',df['train'])\n",
    "prep_semeval('semeval_test_clean.csv',df['test'])\n",
    "prep_semeval('semeval_val_clean.csv', df['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_train = pd.read_csv(\"dataset/SemEval-2018/semeval_train_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_test = pd.read_csv(\"dataset/SemEval-2018/semeval_test_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_val = pd.read_csv(\"dataset/SemEval-2018/semeval_val_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISEAR, Vikash (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear = pd.read_csv('dataset/ISEAR/isear_clean.csv', index_col=0)\n",
    "train, testval= train_test_split(isear, test_size=0.20, random_state=0, stratify=isear[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train = train.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_test = test.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_val = val.rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_train.csv')\n",
    "test.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_test.csv')\n",
    "val.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CARER Emotion Dataset, Elvis et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "class_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "def map_to_label(dataset):\n",
    "    col = []\n",
    "    for l in dataset['label']:\n",
    "        col.append(class_names[l])\n",
    "    return col\n",
    "\n",
    "test_l = map_to_label(dataset['test'])\n",
    "validation_l = map_to_label(dataset['validation'])\n",
    "train_l = map_to_label(dataset['train'])\n",
    "\n",
    "carer_test = pd.DataFrame({\"sentence\": dataset['test']['text'], 'label': test_l, \"emotion_num\": dataset['test']['label']})\n",
    "carer_test.to_csv(\"dataset/Emotion_Elvis/EmoElvis_test_clean.csv\")\n",
    "\n",
    "carer_val = pd.DataFrame({\"sentence\": dataset['validation']['text'], 'label': validation_l, \"emotion_num\": dataset['validation']['label']})\n",
    "carer_val.to_csv(\"dataset/Emotion_Elvis/EmoElvis_validation_clean.csv\")\n",
    "\n",
    "carer_train = pd.DataFrame({\"sentence\": dataset['train']['text'], 'label': train_l, \"emotion_num\": dataset['train']['label']})\n",
    "carer_train.to_csv(\"dataset/Emotion_Elvis/EmoElvis_train_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crowdflower (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/tlkh/text-emotion-classification/master/dataset/original/text_emotion.csv\"\n",
    "flower = load_dataset(\"csv\", data_files=url )\n",
    "X = flower['train']\n",
    "y = flower['train']['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "train_index, test_index = next(sss.split(X, y))\n",
    "\n",
    "train=X[train_index]\n",
    "testval = X[test_index]\n",
    "testval = Dataset.from_dict(testval)\n",
    "\n",
    "X = testval\n",
    "y = testval['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n",
    "val_index, test_index = next(sss.split(X, y))\n",
    "val=X[val_index]\n",
    "test=X[test_index]\n",
    "\n",
    "flower_train = Dataset.from_dict(train).rename(columns={\"sentiment\":'label', 'utterence':'sentence'})\n",
    "flower_test = Dataset.from_dict(test).rename(columns={\"sentiment\":'label', 'utterence':'sentence'})\n",
    "flower_val = Dataset.from_dict(val).rename(columns={\"sentiment\":'label', 'utterence':'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_train.to_csv(\"dataset/crowdflower_data/crowdflower_trian.csv\")\n",
    "flower_test.to_csv(\"dataset/crowdflower_data/crowdflower_test.csv\")\n",
    "flower_val.to_csv(\"dataset/crowdflower_data/crowdflower_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[meld_train,semeval_train,isear_train,carer_train,flower_train],[\"meld\",\"semeval\",\"isear\",\"carer\",\"flower\"])\n",
    "val_final = map_and_concat(val_final,[meld_val,semeval_val,isear_val,carer_val,flower_val],[\"meld\",\"semeval\",\"isear\",\"carer\",\"flower\"])\n",
    "test_final = map_and_concat(test_final,[meld_test,semeval_test,isear_test,carer_test,flower_test],[\"meld\",\"semeval\",\"isear\",\"carer\",\"flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Import, Cleaning, Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
