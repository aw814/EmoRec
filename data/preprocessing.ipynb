{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(imported_dataset):\n",
    "    return imported_dataset['label'].value_counts().reset_index(name = 'count').rename(columns={'index':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis(dataset, plt_title = 'Label Distribution'):\n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(14,5), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'], dataset['count'], width=0.6)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=10, labelpad=11) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=12, labelrotation=0)\n",
    "    labels = dataset['label']\n",
    "    ax.set_xticks(dataset['label'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['label'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['label'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis_large(dataset, plt_title = 'Label Distribution'):\n",
    "    \n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(25,3), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'], dataset['count'], width=0.2)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=8, labelpad=12) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=8, labelrotation=0)\n",
    "    labels = dataset['label']\n",
    "    ax.set_xticks(dataset['label'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['label'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['label'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'ang': \"anger\", 'angry': \"anger\", 'annoyed': \"anger\", 'furious': \"anger\", 'fru': \"anger\", 'annoyance': 'anger', \"disapproval\": 'anger',\n",
    "           'exc': \"joy\", 'joyful': \"joy\", 'happiness': \"joy\", 'hap': \"joy\", 'grateful': \"joy\", 'impressed': \"joy\", 'content': \"joy\", 'fun': \"joy\", 'enthusiasm': \"joy\",\n",
    "           'excited': \"joy\", 'excitement': 'joy', \"pride\": 'joy', 'gratitude': 'joy', \"approval\": 'joy', 'admiration': 'joy', 'proud': 'joy',\n",
    "           'fea': \"fear\", 'terrified': \"fear\", 'afraid': \"fear\",\n",
    "           'disgusted': 'disgust', 'hate': 'disgust', 'boredom': 'disgust',\n",
    "           'neu': \"neutral\",\n",
    "           'sad': \"sadness\", 'devastated': \"sadness\", 'disappointed': \"sadness\", \"grief\": \"sadness\", \"lonely\": 'sadness', 'disappointment': 'sadness',\n",
    "           'sur': \"surprise\", 'surprised': \"surprise\", 'sup': \"surprise\", 'realization': 'surprise',\n",
    "           'hope': \"optimism\", 'faithful': \"optimism\", 'hopeful': 'optimism', 'confident': 'optimism', 'prepared': 'optimism',\n",
    "           'guilty': \"guilt\", 'shame': \"guilt\", 'ashamed': \"guilt\", 'embarrassed': \"guilt\",\n",
    "           'caring': \"love\",\n",
    "           'anxious': \"anxiety\", 'worry': \"anxiety\", 'apprehensive': \"anxiety\", 'nervousness': 'anxiety',\n",
    "           'anticipating': 'anticipation',\n",
    "           'amusement': 'amusement',\n",
    "           'neu': 'neutral',\n",
    "           'confusion': 'curiosity'\n",
    "           }\n",
    "\n",
    "ekman_mapping = {'anticipation': 'tbd',\n",
    "                 'anxiety': \"fear\",\n",
    "                 'empty': 'tbd',\n",
    "                 'guilt': \"sadness\",\n",
    "                 'love': 'joy',\n",
    "                 'optimism': 'joy',\n",
    "                 'peaceful': 'joy',\n",
    "                 'powerful': 'tbd',\n",
    "                 'pessimism': 'sadness',\n",
    "                 'relief': 'tbd'}\n",
    "\n",
    "\n",
    "class MyDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return key\n",
    "\n",
    "\n",
    "def map_and_concat(df, list_of_datasets, list_of_ds_name):\n",
    "    total_rows = 0\n",
    "    for d, name in zip(list_of_datasets, list_of_ds_name):\n",
    "        print(name)\n",
    "        emo_labels = d['label']\n",
    "        d['label'] = emo_labels.map(MyDict(mapping))\n",
    "        ds = d[['sentence', 'label']]\n",
    "        filtered_ds = ds[~ds['label'].str.contains('trust', case=False)]\n",
    "        filtered_ds['ds_name'] = name\n",
    "        df = pd.concat([df, filtered_ds])\n",
    "        total_rows = total_rows + ds.shape[0]\n",
    "        print(total_rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sec_map_and_concat_ekman(d):\n",
    "    total_rows = 0\n",
    "    emo_labels = d['label']\n",
    "    d['label'] = emo_labels.map(MyDict(ekman_mapping))\n",
    "    ds = d[['sentence', 'label', 'ds_name']]\n",
    "    total_rows = total_rows + ds.shape[0]\n",
    "    print(total_rows)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "val_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "test_final = pd.DataFrame(columns=['sentence','label','ds_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b=\"./dataset/train_basic.json\"\n",
    "val_b=\"./dataset/val_basic.json\"\n",
    "test_b=\"./dataset/test_basic.json\"\n",
    "\n",
    "train_no_anger_f4=\"./dataset/train_no_anger_f4.json\"\n",
    "test_no_anger_f4=\"./dataset/test_no_anger_f4.json\"\n",
    "val_no_anger_f4=\"./dataset/val_no_anger_f4.json\"\n",
    "\n",
    "train_b_sample=\"./dataset/train_basic_sample.json\"\n",
    "val_b_sample=\"./dataset/val_basic_sample.json\"\n",
    "test_b_sample=\"./dataset/test_basic_sample.json\"\n",
    "\n",
    "train_b_beam=\"./dataset/train_basic_beam.json\"\n",
    "val_b_beam=\"./dataset/val_basic_beam.json\"\n",
    "test_b_beam=\"./dataset/test_basic_beam.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needs to be lower letter\n",
    "def relabel_emotion(emotion):\n",
    "    if isinstance(emotion, int):\n",
    "        # for daily dialouge\n",
    "        if emotion in [0] : return 'neutral'\n",
    "        if emotion in [1] : return 'anger'\n",
    "        if emotion in [2] : return 'disgust'\n",
    "        if emotion in [3] : return 'fear'\n",
    "        if emotion in [4] : return 'joy'\n",
    "        if emotion in [5] : return 'sadness'\n",
    "        if emotion in [6] : return 'surprise'\n",
    "    else:\n",
    "        emotion = emotion.lower()\n",
    "        if emotion in ['mad','angry', 1, 'anger'] : return 'anger'\n",
    "        if emotion in ['fear','scared', 3] : return 'fear'\n",
    "        if emotion in ['joy','happy','joyful','happiness', 4] : return 'joy'\n",
    "        if emotion in ['sadness','sad', 5] : return 'sadness'\n",
    "        else : return emotion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Import, Cleaning, Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoEmotion (hartmann)\n",
    "https://github.com/google-research/google-research/tree/master/goemotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_go_emo(dataset):\n",
    "    dataset = dataset.dropna(subset=['emotion_label'])\n",
    "    for idx, row in dataset.iterrows():\n",
    "        emotion_label_txt = row['emotion_label']\n",
    "        emotion_label_list = [int(num) for num in emotion_label_txt.split(\",\")]\n",
    "        row['emotion_label'] = emotion_label_list\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_mapping = {\n",
    "  '0': 'admiration',\n",
    "  '1': 'amusement',\n",
    "  '2': 'anger',\n",
    "  '3': 'annoyance',\n",
    "  '4': 'approval',\n",
    "  '5': 'caring',\n",
    "  '6': 'confusion',\n",
    "  '7': 'curiosity',\n",
    "  '8': 'desire',\n",
    "  '9': 'disappointment',\n",
    "  '10': 'disapproval',\n",
    "  '11': 'disgust',\n",
    "  '12': 'embarrassment',\n",
    "  '13': 'excitement',\n",
    "  '14': 'fear',\n",
    "  '15': 'gratitude',\n",
    "  '16': 'grief',\n",
    "  '17': 'joy',\n",
    "  '18': 'love',\n",
    "  '19': 'nervousness',\n",
    "  '20': 'optimism',\n",
    "  '21': 'pride',\n",
    "  '22': 'realization',\n",
    "  '23': 'relief',\n",
    "  '24': 'remorse',\n",
    "  '25': 'sadness',\n",
    "  '26': 'surprise',\n",
    "  '27': 'neutral'\n",
    "}\n",
    "\n",
    "def map_haru_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        if 1 in e:\n",
    "            final_result_emo.append(\"amusement\")\n",
    "        elif 6 in e or 7 in e:\n",
    "            final_result_emo.append(\"curiosity\")\n",
    "        elif 12 in e or 24 in e:\n",
    "            final_result_emo.append(\"guilt\")\n",
    "        elif 20 in e:\n",
    "            final_result_emo.append(\"optimism\")\n",
    "        elif 5 in e or 18 in e:\n",
    "            final_result_emo.append(\"love\")    \n",
    "        else:\n",
    "            l = e[random.randint(0,len(e)-1)]\n",
    "            final_result_emo.append(orig_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_fine = map_haru_emotion(format_go_emo(train)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_fine = map_haru_emotion(format_go_emo(test)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_fine = map_haru_emotion(format_go_emo(val)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_fine_goemo.csv\")\n",
    "test_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_fine_goemo.csv\")\n",
    "val_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_fine_goemo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ekman (mapped all fine-grained to Ekman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekman_mapping = {\n",
    "\"0\" : \"anger\",\n",
    "\"1\" : \"disgust\",\n",
    "\"2\" : \"fear\",\n",
    "\"3\" : \"joy\",\n",
    "\"4\" : \"neutral\",\n",
    "\"5\" : \"sadness\",\n",
    "\"6\" : \"surprise\"\n",
    "}\n",
    "def map_ekman_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        l = e[random.randint(0,len(e)-1)]\n",
    "        final_result_emo.append(ekman_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_ekman = map_ekman_emotion(format_go_emo(train_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_ekman = map_ekman_emotion(format_go_emo(test_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_ekman = map_ekman_emotion(format_go_emo(val_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_fine_goemo.csv\")\n",
    "test_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_fine_goemo.csv\")\n",
    "val_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_fine_goemo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goemotion\n",
      "43410\n",
      "goemotion\n",
      "5426\n",
      "goemotion\n",
      "5427\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train_ekman],['goemotion'])\n",
    "val_final = map_and_concat(val_final,[val_ekman],['goemotion'])\n",
    "test_final = map_and_concat(test_final,[test_ekman],['goemotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmoryNLP (BM)\n",
    "[link] https://github.com/emorynlp/emotion-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-cecdd3f673368c60/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a48b14daf6462687faf9bcd224d45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-53f09f25d41a1e93/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f83853ee88147f7861c26410c743def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/annie/.cache/huggingface/datasets/json/default-c3a8fb9e451079f1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45a5e21cdd843afbce056bafb5b28e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emonlp_train = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-trn.json\" }, field=\"episodes\")\n",
    "emonlp_val = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-dev.json\" }, field=\"episodes\")\n",
    "emonlp_test = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-tst.json\" }, field=\"episodes\")\n",
    "\n",
    "\n",
    "def format_emonlp(emonlp_dataset):\n",
    "    utterances_emonlp = []\n",
    "    emotions_emonlp = []\n",
    "    for row in emonlp_dataset['train']:\n",
    "        scenes = row['scenes']\n",
    "        for scene in scenes:\n",
    "            utterances = scene['utterances']\n",
    "            for utterance in utterances:\n",
    "                # extract only text and emotion labels\n",
    "                utterances_emonlp.append(utterance['transcript'])\n",
    "                emotions_emonlp.append(relabel_emotion(utterance['emotion']))\n",
    "    clean_emonlp = {}\n",
    "    clean_emonlp[\"sentence\"] = utterances_emonlp\n",
    "    clean_emonlp[\"label\"] = emotions_emonlp\n",
    "    clean_emonlp = Dataset.from_dict(clean_emonlp).filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_emonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22ecc5c22724811b5eb91244dd63b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc007581be040f0ba927208d1c6ab73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfdceada9a548b4946b6b2684647e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_emonlp = format_emonlp(emonlp_train).to_pandas()\n",
    "val_emonlp = format_emonlp(emonlp_val).to_pandas()\n",
    "test_emonlp = format_emonlp(emonlp_test).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emonlp.to_csv('dataset/Emorynlp/emonlp_train_clean.csv', index = False)\n",
    "val_emonlp.to_csv('dataset/Emorynlp/emonlp_val_clean.csv', index = False)\n",
    "test_emonlp.to_csv('dataset/Emorynlp/emonlp_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoryNLP\n",
      "9934\n",
      "emoryNLP\n",
      "1344\n",
      "emoryNLP\n",
      "1328\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train_emonlp],[\"emoryNLP\"])\n",
    "val_final = map_and_concat(val_final,[val_emonlp],[\"emoryNLP\"])\n",
    "test_final = map_and_concat(test_final,[test_emonlp],[\"emoryNLP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empathetic Dialogues (excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DailyDialog (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (/home/annie/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5733413a91478980be2cb25dbfbd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d735186d3ed249e0badd79bfa4eb3787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dabd2e4d20141c5aa816e0ca8896f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9405bc731a4f439427342981333ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "daily = load_dataset(\"daily_dialog\")\n",
    "\n",
    "def format_dd(daily):\n",
    "    utterances_daily = []\n",
    "    for d in daily[\"dialog\"]:\n",
    "        utterances_daily = utterances_daily + d\n",
    "    emotions_daily = []\n",
    "    for d in daily[\"emotion\"]:\n",
    "        emotions_daily = emotions_daily + d\n",
    "    clean_daily = {}\n",
    "    clean_daily[\"sentence\"] = utterances_daily\n",
    "    clean_daily[\"label\"] = []\n",
    "    for emo in emotions_daily : clean_daily[\"label\"].append(relabel_emotion(emo))\n",
    "    clean_daily = Dataset.from_dict(clean_daily)\n",
    "    clean_daily = clean_daily.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_daily\n",
    "\n",
    "train_dd = format_dd(daily['train']).to_pandas()\n",
    "val_dd = format_dd(daily['validation']).to_pandas()\n",
    "test_dd = format_dd(daily['test']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily\n",
      "87170\n",
      "daily\n",
      "8069\n",
      "daily\n",
      "7740\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train_dd],[\"daily\"])\n",
    "val_final = map_and_concat(val_final,[val_dd],[\"daily\"])\n",
    "test_final = map_and_concat(test_final,[test_dd],[\"daily\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEMOCAP (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_utterance = []\n",
    "def split_string_u(string):\n",
    "    pattern = r'^(.*?)(\\[[\\d.-]+\\]:\\s*)(.*)$'\n",
    "    match = re.match(pattern, string)\n",
    "    \n",
    "    if match:\n",
    "        part1 = match.group(1).rstrip()  # Remove trailing spaces from the first part\n",
    "        part2 = match.group(2).rstrip().rstrip(\":\")  # Remove trailing spaces from the second part\n",
    "        part3 = match.group(3)\n",
    "        return part1, part2, part3\n",
    "    else:\n",
    "        excluded_utterance.append(string)\n",
    "\n",
    "def split_string_e(string):\n",
    "   return string.strip().split('\\t')\n",
    "def read_transcription(path):\n",
    "    transcript_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    utterance_list = []\n",
    "    for u in transcript_ds['test']['text']:\n",
    "        utterance = split_string_u(u)\n",
    "        utterance_list.append(utterance)\n",
    "    return utterance_list\n",
    "\n",
    "def read_emotion(path):\n",
    "    emo_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    print(path + '\\n')\n",
    "    emotion_list = []\n",
    "    for i,e in enumerate(emo_ds['test']['text']):\n",
    "        if (e.strip() == '') and (i + 1 < len(emo_ds['test']['text'])):\n",
    "            emotion = split_string_e(emo_ds['test']['text'][i+1])\n",
    "            emotion_list.append(emotion)        \n",
    "    return emotion_list\n",
    "\n",
    "def read_iemocap(dir_path):\n",
    "    # initial two result df to store all transcriptions from all sessions\n",
    "    # similarly for emotions\n",
    "    transcriptions_df = pd.DataFrame(columns=['idx', 'labs', 'utterance', 'session'])\n",
    "    emotions_df = pd.DataFrame(columns=['labs', 'idx', 'emotion', 'attribute', 'session'])\n",
    "    sessions = ['Session1','Session2','Session3','Session4', 'Session5']\n",
    "    for session in sessions:\n",
    "        # get all transcriptions\n",
    "        transcriptions = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/transcriptions'\n",
    "        all_utterances = []\n",
    "        for item in os.listdir(transcriptions):\n",
    "            item_path = os.path.join(transcriptions, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_utterances = all_utterances + read_transcription(item_path)\n",
    "        u_df = pd.DataFrame(all_utterances, columns=['idx', 'labs', 'utterance'])\n",
    "        u_df['session'] = session\n",
    "        transcriptions_df = pd.concat([transcriptions_df, u_df])\n",
    "        \n",
    "        # get all emotions\n",
    "        emoeval = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/EmoEvaluation'\n",
    "        all_emotions = []\n",
    "        for item in os.listdir(emoeval):\n",
    "            item_path = os.path.join(emoeval, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_emotions = all_emotions + read_emotion(item_path)\n",
    "        e_df = pd.DataFrame(all_emotions, columns=['labs', 'idx', 'emotion', 'attribute']) \n",
    "        e_df['session'] = session\n",
    "        emotions_df = pd.concat([emotions_df, e_df])\n",
    "\n",
    "    return {'utterance':transcriptions_df, \n",
    "            'emotion': emotions_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iemocap_dir = 'dataset/IEMOCAP_full_release_withoutVideos'\n",
    "# join by idx\n",
    "read_results = read_iemocap(iemocap_dir)\n",
    "clean_imocap = pd.merge(read_results['utterance'],read_results['emotion'], on='idx')\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'xxx']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'dis']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'oth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test val split: stratify base on labels\n",
    "train, testval= train_test_split(clean_imocap, test_size=0.20, random_state=0, stratify=clean_imocap[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n",
    "\n",
    "train.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_train.csv')\n",
    "test.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_test.csv')\n",
    "val.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_train.csv')\n",
    "test = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_test.csv')\n",
    "val = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iemocap\n",
      "6021\n",
      "iemocap\n",
      "753\n",
      "iemocap\n",
      "753\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[train],[\"iemocap\"])\n",
    "val_final = map_and_concat(val_final,[val],[\"iemocap\"])\n",
    "test_final = map_and_concat(test_final,[test],[\"iemocap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MELD (BM) (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/annie/.cache/huggingface/datasets/csv/default-9a6afb493ce7fdae/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0550892925a468cadecdd52c970f304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meld = load_dataset(\"csv\", data_files={\"test\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/test_sent_emo.csv\",\n",
    "                                       \"train\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/train_sent_emo.csv\",\n",
    "                                       \"val\": \"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/dev_sent_emo.csv\"})\n",
    "\n",
    "def format_meld(meld):\n",
    "    clean_meld = {}\n",
    "    clean_meld[\"sentence\"] = meld[\"Utterance\"]\n",
    "\n",
    "    clean_meld[\"label\"] = []\n",
    "    for emo in meld[\"Emotion\"] : clean_meld[\"label\"].append(relabel_emotion(emo))\n",
    "    clean_meld = Dataset.from_dict(clean_meld)\n",
    "    clean_meld = clean_meld.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf76165f70c4020ac37f04d1350f6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8002761dc149f2a236fcdb8c0cafc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1109 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f9606640a449abaaf8c58199513324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meld_train = format_meld(meld['train']).to_pandas()\n",
    "meld_val = format_meld(meld['val']).to_pandas()\n",
    "meld_test = format_meld(meld['test']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train.to_csv('dataset/MELD/meld_train_clean.csv', index = False)\n",
    "meld_val.to_csv('dataset/MELD/meld_val_clean.csv', index = False)\n",
    "meld_test.to_csv('dataset/MELD/meld_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemEval-2018, EI-reg, Mohammad et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_semeval(filename, dataset):\n",
    "    new_df = pd.DataFrame(columns=['ID', 'utterance', 'emotion'],dtype=object)\n",
    "    for index, row in dataset.iterrows():\n",
    "        utterance = row['Tweet']\n",
    "        emotions_col = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "        id = index\n",
    "        emotions_list = []\n",
    "        for emotion in emotions_col:\n",
    "            if row[emotion] == 1:\n",
    "                emotions_list.append(emotion) \n",
    "        \n",
    "        if emotions_list: \n",
    "            e = emotions_list[random.randint(0,len(emotions_list)-1)]\n",
    "            new_row = {\"ID\": id,\n",
    "                    \"utterance\": utterance,\n",
    "                    \"emotion\": e}\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    new_df.to_csv(\"dataset/SemEval-2018/\" + filename, index=False)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n",
    "prep_semeval('semeval_train_clean.csv',df['train'].to_pandas())\n",
    "prep_semeval('semeval_test_clean.csv',df['test'].to_pandas())\n",
    "prep_semeval('semeval_val_clean.csv', df['validation'].to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_train = pd.read_csv(\"dataset/SemEval-2018/semeval_train_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_test = pd.read_csv(\"dataset/SemEval-2018/semeval_test_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_val = pd.read_csv(\"dataset/SemEval-2018/semeval_val_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISEAR, Vikash (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear = pd.read_csv('dataset/ISEAR/isear_clean.csv', index_col=0)\n",
    "train, testval= train_test_split(isear, test_size=0.20, random_state=0, stratify=isear[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train = train.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_test = test.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_val = val.rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_train.csv')\n",
    "isear_test.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_test.csv')\n",
    "isear_val.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_train.csv').rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_test = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_test.csv').rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_val = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_val.csv').rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CARER Emotion Dataset, Elvis et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/annie/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb4c6357f8a444aa8ca5acccdf5616d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "class_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "def map_to_label(dataset):\n",
    "    col = []\n",
    "    for l in dataset['label']:\n",
    "        col.append(class_names[l])\n",
    "    return col\n",
    "\n",
    "test_l = map_to_label(dataset['test'])\n",
    "validation_l = map_to_label(dataset['validation'])\n",
    "train_l = map_to_label(dataset['train'])\n",
    "\n",
    "carer_test = pd.DataFrame({\"sentence\": dataset['test']['text'], 'label': test_l, \"emotion_num\": dataset['test']['label']})\n",
    "carer_test.to_csv(\"dataset/Emotion_Elvis/EmoElvis_test_clean.csv\")\n",
    "\n",
    "carer_val = pd.DataFrame({\"sentence\": dataset['validation']['text'], 'label': validation_l, \"emotion_num\": dataset['validation']['label']})\n",
    "carer_val.to_csv(\"dataset/Emotion_Elvis/EmoElvis_validation_clean.csv\")\n",
    "\n",
    "carer_train = pd.DataFrame({\"sentence\": dataset['train']['text'], 'label': train_l, \"emotion_num\": dataset['train']['label']})\n",
    "carer_train.to_csv(\"dataset/Emotion_Elvis/EmoElvis_train_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crowdflower (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/annie/.cache/huggingface/datasets/csv/default-978bd5f0a2273fda/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892554706b6147b9bc2d1a97ef1fce75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/tlkh/text-emotion-classification/master/dataset/original/text_emotion.csv\"\n",
    "flower = load_dataset(\"csv\", data_files=url )\n",
    "X = flower['train']\n",
    "y = flower['train']['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "train_index, test_index = next(sss.split(X, y))\n",
    "\n",
    "train=X[train_index]\n",
    "testval = X[test_index]\n",
    "testval = Dataset.from_dict(testval)\n",
    "\n",
    "X = testval\n",
    "y = testval['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n",
    "val_index, test_index = next(sss.split(X, y))\n",
    "val=X[val_index]\n",
    "test=X[test_index]\n",
    "\n",
    "flower_train = pd.DataFrame.from_dict(train).rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_test = pd.DataFrame.from_dict(test).rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_val = pd.DataFrame.from_dict(val).rename(columns={\"sentiment\":'label', 'content':'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_train.to_csv(\"dataset/crowdflower_data/crowdflower_trian.csv\")\n",
    "flower_test.to_csv(\"dataset/crowdflower_data/crowdflower_test.csv\")\n",
    "flower_val.to_csv(\"dataset/crowdflower_data/crowdflower_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower\n",
      "32000\n",
      "flower\n",
      "4000\n",
      "flower\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[flower_train],[\"flower\"])\n",
    "val_final = map_and_concat(val_final,[flower_val],[\"flower\"])\n",
    "test_final = map_and_concat(test_final,[flower_test],[\"flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meld\n",
      "9989\n",
      "semeval\n",
      "16623\n",
      "isear\n",
      "22755\n",
      "carer\n",
      "38755\n",
      "meld\n",
      "1109\n",
      "semeval\n",
      "1981\n",
      "isear\n",
      "2748\n",
      "carer\n",
      "4748\n",
      "meld\n",
      "2610\n",
      "semeval\n",
      "5794\n",
      "isear\n",
      "6561\n",
      "carer\n",
      "8561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103413/2637432193.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ds['ds_name'] = name\n",
      "/tmp/ipykernel_103413/2637432193.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ds['ds_name'] = name\n",
      "/tmp/ipykernel_103413/2637432193.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ds['ds_name'] = name\n"
     ]
    }
   ],
   "source": [
    "train_final = map_and_concat(train_final,[meld_train,semeval_train,isear_train,carer_train],[\"meld\",\"semeval\",\"isear\",\"carer\"])\n",
    "val_final = map_and_concat(val_final,[meld_val,semeval_val,isear_val,carer_val],[\"meld\",\"semeval\",\"isear\",\"carer\"])\n",
    "test_final = map_and_concat(test_final,[meld_test,semeval_test,isear_test,carer_test],[\"meld\",\"semeval\",\"isear\",\"carer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final.to_json('dataset/train_final_temp.json',orient='records')\n",
    "# test_final.to_json('dataset/test_final_temp.json',orient='records')\n",
    "# val_final.to_json('dataset/val_final_temp.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "val_basic = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "test_basic = pd.DataFrame(columns=['sentence','label','ds_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217171\n",
      "24327\n",
      "27758\n"
     ]
    }
   ],
   "source": [
    "train_basic=sec_map_and_concat_ekman(train_final)\n",
    "val_basic=sec_map_and_concat_ekman(val_final)\n",
    "test_basic=sec_map_and_concat_ekman(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic=train_basic[train_basic['label']!='tbd']\n",
    "val_basic=val_basic[val_basic['label']!='tbd']\n",
    "test_basic=test_basic[test_basic['label']!='tbd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic.to_json(train_b,orient='records')\n",
    "val_basic.to_json(val_b,orient='records')\n",
    "test_basic.to_json(test_b,orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = ['daily', 'emoryNLP',\n",
    "           'iemocap']\n",
    "train_temp = train_basic[train_basic['ds_name'].isin(dataset_name_list)]\n",
    "val_temp = val_basic[val_basic['ds_name'].isin(dataset_name_list)]\n",
    "test_temp = test_basic[test_basic['ds_name'].isin(dataset_name_list)]\n",
    "t_anger = train_temp[train_temp['label'] == 'anger']\n",
    "tst_anger = test_temp[test_temp['label'] == 'anger']\n",
    "v_anger = val_temp[val_temp['label'] == 'anger']\n",
    "\n",
    "train_no_anger = train_basic.drop(index=t_anger.index)\n",
    "val_no_anger = val_basic.drop(index=v_anger.index)\n",
    "test_no_anger = test_basic.drop(index=tst_anger.index)\n",
    "\n",
    "train_no_anger.to_json(train_no_anger_f4, orient=\"records\")\n",
    "val_no_anger.to_json(test_no_anger_f4,orient='records')\n",
    "test_no_anger.to_json(test_no_anger_f4,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic = pd.read_json(train_b)\n",
    "val_basic = pd.read_json(val_b)\n",
    "test_basic = pd.read_json(test_b)\n",
    "all_basic = pd.concat([train_basic,val_basic,test_basic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = count_labels(train_basic)\n",
    "v = count_labels(val_basic)\n",
    "tst = count_labels(test_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_summary = pd.DataFrame({\n",
    "    \"label\":tr['label'],\n",
    "    'tr': tr['count'],\n",
    "    'val': v['count'],\n",
    "    'tst':tst['count']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_summary.to_csv('dataset/labels_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(train_b)\n",
    "val = pd.read_json(val_b)\n",
    "test = pd.read_json(test_b)\n",
    "\n",
    "train_agg = count_labels(train)\n",
    "train_avg = round(train_agg['count'].mean())\n",
    "test_agg = count_labels(test)\n",
    "test_avg = round(test_agg['count'].mean())\n",
    "val_agg = count_labels(val)\n",
    "val_avg = round(val_agg['count'].mean())\n",
    "\n",
    "\n",
    "list = [(train, train_agg, train_avg),(test, test_agg, test_avg), (val, val_agg, val_avg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over+UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_basic_sampling(df, df_agg, avg):\n",
    "    result_df = pd.DataFrame()\n",
    "    undersampled_dfs = df_agg[df_agg['count'] >= avg]['label'] \n",
    "    for ud_df in undersampled_dfs:\n",
    "        u =  df[df['label'] == ud_df].sample(avg)\n",
    "        result_df = pd.concat([result_df,u])\n",
    "        \n",
    "    oversampled_dfs = df_agg[df_agg['count'] < avg]['label'] \n",
    "    for ov_df in oversampled_dfs:\n",
    "        o =  df[df['label'] == ov_df].sample(avg, replace=True)\n",
    "        result_df = pd.concat([result_df,o])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_basic_sampling(list[0][0],list[0][1],list[0][2]).to_json(train_b_sample, orient='records')\n",
    "do_basic_sampling(list[1][0],list[1][1],list[1][2]).to_json(test_b_sample, orient='records')\n",
    "do_basic_sampling(list[2][0],list[2][1],list[2][2]).to_json(val_b_sample, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT Diverse Beam Search for Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams,\n",
    "    num_return_sequences,\n",
    "    num_beam_groups = 2,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generative_sampling(df, df_agg, avg):\n",
    "    result_df = pd.DataFrame()\n",
    "    undersampled_dfs = df_agg[df_agg['count'] >= avg]['label'] \n",
    "    for ud_df in undersampled_dfs:\n",
    "        u =  df[df['label'] == ud_df].sample(avg)\n",
    "        result_df = pd.concat([result_df,u])\n",
    "        \n",
    "    oversampled_dfs = df_agg[df_agg['count'] < avg]['label'] \n",
    "    for ov_df in oversampled_dfs:\n",
    "        label_count = df_agg[df_agg['label'] == ov_df]['count']\n",
    "        difference = math.ceil(avg-label_count)\n",
    "        num_candidates_per_sent = math.ceil((difference)/label_count)\n",
    "        num_beams = num_candidates_per_sent * 2\n",
    "        num_return_sequences = num_candidates_per_sent\n",
    "        print(ov_df)\n",
    "        print('--------- num_candidates_per_sent --------\\n')\n",
    "        print(num_candidates_per_sent)   \n",
    "        sentences = df[df['label'] == ov_df]['sentence']\n",
    "        sentences_to_paraphrase = sentences\n",
    "        if num_candidates_per_sent == 1:\n",
    "            sentences_to_paraphrase = sentences.sample(difference)\n",
    "        result_generated_sentences = []\n",
    "        for sent in sentences_to_paraphrase: \n",
    "            sent_paraphrase_candidates = paraphrase(sent,num_beams,num_return_sequences)\n",
    "            sent_paraphrases_list = sent_paraphrase_candidates[:num_candidates_per_sent]\n",
    "            result_generated_sentences = result_generated_sentences + sent_paraphrases_list\n",
    "        result_sentences = sentences.to_list() + result_generated_sentences\n",
    "        result_dict = dict(sentence=result_sentences, label=ov_df)\n",
    "        o = pd.DataFrame.from_dict(result_dict)\n",
    "        result_df = pd.concat([result_df,o])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_sampling(list[0][0],list[0][1],list[0][2]).to_json(test_b_beam, orient='records')\n",
    "generative_sampling(list[1][0],list[1][1],list[1][2]).to_json(test_b_beam, orient='records')\n",
    "generative_sampling(list[2][0],list[2][1],list[2][2]).to_json(val_b_beam, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
