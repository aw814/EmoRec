{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(imported_dataset):\n",
    "    return imported_dataset['label'].value_counts().reset_index(name = 'count').rename(columns={'index':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis(dataset, plt_title = 'Label Distribution'):\n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(14,5), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'], dataset['count'], width=0.6)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=10, labelpad=11) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=12, labelrotation=0)\n",
    "    labels = dataset['label']\n",
    "    ax.set_xticks(dataset['label'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['label'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['label'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dis_large(dataset, plt_title = 'Label Distribution'):\n",
    "    \n",
    "    dataset = count_labels(dataset)\n",
    "    # Create the figure and axes objects, specify the size and the dots per inches \n",
    "    fig, ax = plt.subplots(figsize=(25,3), dpi = 96)\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'], dataset['count'], width=0.2)\n",
    "\n",
    "    # Create the grid \n",
    "    ax.grid(which=\"major\", axis='x', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "    ax.grid(which=\"major\", axis='y', color='#DAD8D7', alpha=0.5, zorder=1)\n",
    "\n",
    "    # Reformat x-axis label and tick labels\n",
    "    ax.set_xlabel('', fontsize=8, labelpad=12) # No need for an axis label\n",
    "    ax.xaxis.set_label_position(\"bottom\")\n",
    "    ax.xaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.xaxis.set_tick_params(pad=2, labelbottom=True, bottom=True, labelsize=8, labelrotation=0)\n",
    "    labels = dataset['label']\n",
    "    ax.set_xticks(dataset['label'], labels) # Map integers numbers from the series to labels list\n",
    "\n",
    "    # Reformat y-axis\n",
    "    ax.set_ylabel('Count', fontsize=10, labelpad=11)\n",
    "    ax.yaxis.set_label_position(\"left\")\n",
    "    ax.yaxis.set_major_formatter(lambda s, i : f'{s:,.0f}')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_tick_params(pad=2, labeltop=False, labelbottom=True, bottom=False, labelsize=12)\n",
    "\n",
    "    # Add label on top of each bar\n",
    "    ax.bar_label(bar1, labels=[f'{e:,.1f}' for e in dataset['count']], padding=3, color='black', fontsize=8) \n",
    "\n",
    "    # Add in red line and rectangle on top\n",
    "    ax.plot([0.12, .9], [.98, .98], transform=fig.transFigure, clip_on=False, color='#E3120B', linewidth=.6)\n",
    "    ax.add_patch(plt.Rectangle((0.12,.98), 0.04, -0.02, facecolor='#E3120B', transform=fig.transFigure, clip_on=False, linewidth = 0))\n",
    "\n",
    "    # Add in title and subtitle\n",
    "    ax.text(x=0.12, y=.93, s=plt_title, transform=fig.transFigure, ha='left', fontsize=14, weight='bold', alpha=.8)\n",
    "    ax.text(x=0.12, y=.90, s=\"\", transform=fig.transFigure, ha='left', fontsize=12, alpha=.8)\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colours = [\"#2196f3\", \"#bbdefb\"]\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\"colour_map\", colours, N=256)\n",
    "    norm = mpl.colors.Normalize(dataset['count'].min(), dataset['count'].max()) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "\n",
    "    # Plot bars\n",
    "    bar1 = ax.bar(dataset['label'],dataset['count'], color=cmap(norm(dataset['count'])), width=0.6, zorder=2)\n",
    "\n",
    "    # Find the average data point and split the series in 2\n",
    "    average = dataset['count'].mean()\n",
    "    below_average = dataset[dataset['count']<average]\n",
    "    above_average = dataset[dataset['count']>=average]\n",
    "\n",
    "    # Colours - Choose the extreme colours of the colour map\n",
    "    colors_high = [\"#E1ACAC\", \"#E1ACAC\"] # Extreme colours of the high scale\n",
    "    colors_low = [\"#004B84\",\"#004B84\"] # Extreme colours of the low scale\n",
    "\n",
    "    # Colormap - Build the colour maps\n",
    "    cmap_low = mpl.colors.LinearSegmentedColormap.from_list(\"low_map\", colors_low, N=256)\n",
    "    cmap_high = mpl.colors.LinearSegmentedColormap.from_list(\"high_map\", colors_high, N=256)\n",
    "    norm_low = mpl.colors.Normalize(below_average['count'].min(), average) # linearly normalizes data into the [0.0, 1.0] interval\n",
    "    norm_high = mpl.colors.Normalize(average, above_average['count'].max())\n",
    "\n",
    "    # Plot bars and average (horizontal) line\n",
    "    bar1 = ax.bar(below_average['label'], below_average['count'], color=cmap_low(norm_low(below_average['count'])), width=0.6, label='Below Average', zorder=2)\n",
    "    bar2 = ax.bar(above_average['label'], above_average['count'], color=cmap_high(norm_high(above_average['count'])), width=0.6, label='Above Average', zorder=2)\n",
    "    plt.axhline(y=average, color = 'grey', linewidth=3)\n",
    "\n",
    "    # Determine the y-limits of the plot\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    # Calculate a suitable y position for the text label\n",
    "    y_pos = average/ymax + 0.03\n",
    "    # Annotate the average line\n",
    "    ax.text(0.88, y_pos, f'Average = {average:.1f}', ha='right', va='center', transform=ax.transAxes, size=8, zorder=3)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc=\"best\", ncol=2, bbox_to_anchor=[1, 1.07], borderaxespad=0, frameon=False, fontsize=8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'ang': \"anger\", 'angry': \"anger\", 'annoyed': \"anger\", 'furious': \"anger\", 'fru': \"anger\", 'annoyance': 'anger', \"disapproval\": 'anger',\n",
    "           'exc': \"joy\", 'joyful': \"joy\", 'happiness': \"joy\", 'hap': \"joy\", 'grateful': \"joy\", 'impressed': \"joy\", 'content': \"joy\", 'fun': \"joy\", 'enthusiasm': \"joy\",\n",
    "           'excited': \"joy\", 'excitement': 'joy', \"pride\": 'joy', 'gratitude': 'joy', \"approval\": 'joy', 'admiration': 'joy', 'proud': 'joy',\n",
    "           'fea': \"fear\", 'terrified': \"fear\", 'afraid': \"fear\",\n",
    "           'disgusted': 'disgust', 'hate': 'disgust', 'boredom': 'disgust',\n",
    "           'neu': \"neutral\",\n",
    "           'sad': \"sadness\", 'devastated': \"sadness\", 'disappointed': \"sadness\", \"grief\": \"sadness\", \"lonely\": 'sadness', 'disappointment': 'sadness',\n",
    "           'sur': \"surprise\", 'surprised': \"surprise\", 'sup': \"surprise\", 'realization': 'surprise',\n",
    "           'hope': \"optimism\", 'faithful': \"optimism\", 'hopeful': 'optimism', 'confident': 'optimism', 'prepared': 'optimism',\n",
    "           'guilty': \"guilt\", 'shame': \"guilt\", 'ashamed': \"guilt\", 'embarrassed': \"guilt\",\n",
    "           'caring': \"love\",\n",
    "           'anxious': \"anxiety\", 'worry': \"anxiety\", 'apprehensive': \"anxiety\", 'nervousness': 'anxiety',\n",
    "           'anticipating': 'anticipation',\n",
    "           'amusement': 'amusement',\n",
    "           'neu': 'neutral',\n",
    "           'confusion': 'curiosity'\n",
    "           }\n",
    "\n",
    "ekman_mapping = {'anticipation': 'tbd',\n",
    "                 'anxiety': \"fear\",\n",
    "                 'empty': 'tbd',\n",
    "                 'guilt': \"disgust\",\n",
    "                 'love': 'joy',\n",
    "                 'optimism': 'joy',\n",
    "                 'peaceful': 'tbd',\n",
    "                 'powerful': 'tbd',\n",
    "                 'pessimism': 'sadness',\n",
    "                 'relief': 'tbd'}\n",
    "\n",
    "\n",
    "class MyDict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return key\n",
    "\n",
    "\n",
    "def map_and_concat(df, list_of_datasets, list_of_ds_name):\n",
    "    total_rows = 0\n",
    "    for d, name in zip(list_of_datasets, list_of_ds_name):\n",
    "        print(name)\n",
    "        emo_labels = d['label']\n",
    "        d['label'] = emo_labels.map(MyDict(mapping))\n",
    "        ds = d[['sentence', 'label']]\n",
    "        filtered_ds = ds[~ds['label'].str.contains('trust', case=False)]\n",
    "        filtered_ds['ds_name'] = name\n",
    "        df = pd.concat([df, filtered_ds])\n",
    "        total_rows = total_rows + ds.shape[0]\n",
    "        print(total_rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sec_map_and_concat_ekman(d):\n",
    "    total_rows = 0\n",
    "    emo_labels = d['label']\n",
    "    d['label'] = emo_labels.map(MyDict(ekman_mapping))\n",
    "    ds = d[['sentence', 'label', 'ds_name']]\n",
    "    total_rows = total_rows + ds.shape[0]\n",
    "    print(total_rows)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "val_final = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "test_final = pd.DataFrame(columns=['sentence','label','ds_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b=\"./dataset/train_basic.json\"\n",
    "val_b=\"./dataset/val_basic.json\"\n",
    "test_b=\"./dataset/test_basic.json\"\n",
    "\n",
    "train_b_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/train_basic.json\"\n",
    "val_b_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/val_basic.json\"\n",
    "test_b_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/test_basic.json\"\n",
    "\n",
    "train_no_anger_f4=\"./dataset/train_no_anger_f4.json\"\n",
    "test_no_anger_f4=\"./dataset/test_no_anger_f4.json\"\n",
    "val_no_anger_f4=\"./dataset/val_no_anger_f4.json\"\n",
    "\n",
    "\n",
    "train_no_anger_f4_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/train_no_anger_f4.json\"\n",
    "test_no_anger_f4_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/test_no_anger_f4.json\"\n",
    "val_no_anger_f4_og=\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/val_no_anger_f4.json\"\n",
    "\n",
    "\n",
    "train_b_sample=\"./dataset/train_basic_sample.json\"\n",
    "val_b_sample=\"./dataset/val_basic_sample.json\"\n",
    "test_b_sample=\"./dataset/test_basic_sample.json\"\n",
    "\n",
    "train_b_beam=\"./dataset/train_basic_beam.json\"\n",
    "val_b_beam=\"./dataset/val_basic_beam.json\"\n",
    "test_b_beam=\"./dataset/test_basic_beam.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input needs to be lower letter\n",
    "def relabel_emotion(emotion):\n",
    "    if isinstance(emotion, int):\n",
    "        # for daily dialouge\n",
    "        if emotion in [0] : return 'neutral'\n",
    "        if emotion in [1] : return 'anger'\n",
    "        if emotion in [2] : return 'disgust'\n",
    "        if emotion in [3] : return 'fear'\n",
    "        if emotion in [4] : return 'joy'\n",
    "        if emotion in [5] : return 'sadness'\n",
    "        if emotion in [6] : return 'surprise'\n",
    "    else:\n",
    "        emotion = emotion.lower()\n",
    "        if emotion in ['mad','angry', 1, 'anger'] : return 'anger'\n",
    "        if emotion in ['fear','scared', 3] : return 'fear'\n",
    "        if emotion in ['joy','happy','joyful','happiness', 4] : return 'joy'\n",
    "        if emotion in ['sadness','sad', 5] : return 'sadness'\n",
    "        else : return emotion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Import, Cleaning, Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoEmotion (hartmann)\n",
    "https://github.com/google-research/google-research/tree/master/goemotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_go_emo(dataset):\n",
    "    dataset = dataset.dropna(subset=['emotion_label'])\n",
    "    for idx, row in dataset.iterrows():\n",
    "        emotion_label_txt = row['emotion_label']\n",
    "        emotion_label_list = [int(num) for num in emotion_label_txt.split(\",\")]\n",
    "        row['emotion_label'] = emotion_label_list\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_mapping = {\n",
    "  '0': 'admiration',\n",
    "  '1': 'amusement',\n",
    "  '2': 'anger',\n",
    "  '3': 'annoyance',\n",
    "  '4': 'approval',\n",
    "  '5': 'caring',\n",
    "  '6': 'confusion',\n",
    "  '7': 'curiosity',\n",
    "  '8': 'desire',\n",
    "  '9': 'disappointment',\n",
    "  '10': 'disapproval',\n",
    "  '11': 'disgust',\n",
    "  '12': 'embarrassment',\n",
    "  '13': 'excitement',\n",
    "  '14': 'fear',\n",
    "  '15': 'gratitude',\n",
    "  '16': 'grief',\n",
    "  '17': 'joy',\n",
    "  '18': 'love',\n",
    "  '19': 'nervousness',\n",
    "  '20': 'optimism',\n",
    "  '21': 'pride',\n",
    "  '22': 'realization',\n",
    "  '23': 'relief',\n",
    "  '24': 'remorse',\n",
    "  '25': 'sadness',\n",
    "  '26': 'surprise',\n",
    "  '27': 'neutral'\n",
    "}\n",
    "\n",
    "def map_haru_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        if 1 in e:\n",
    "            final_result_emo.append(\"amusement\")\n",
    "        elif 6 in e or 7 in e:\n",
    "            final_result_emo.append(\"curiosity\")\n",
    "        elif 12 in e or 24 in e:\n",
    "            final_result_emo.append(\"guilt\")\n",
    "        elif 20 in e:\n",
    "            final_result_emo.append(\"optimism\")\n",
    "        elif 5 in e or 18 in e:\n",
    "            final_result_emo.append(\"love\")    \n",
    "        else:\n",
    "            l = e[random.randint(0,len(e)-1)]\n",
    "            final_result_emo.append(orig_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/GoEmotions-pytorch/data/original/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_fine = map_haru_emotion(format_go_emo(train)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_fine = map_haru_emotion(format_go_emo(test)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_fine = map_haru_emotion(format_go_emo(val)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_fine_goemo.csv\")\n",
    "test_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_fine_goemo.csv\")\n",
    "val_fine.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_fine_goemo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ekman (mapped all fine-grained to Ekman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekman_mapping = {\n",
    "\"0\" : \"anger\",\n",
    "\"1\" : \"disgust\",\n",
    "\"2\" : \"fear\",\n",
    "\"3\" : \"joy\",\n",
    "\"4\" : \"neutral\",\n",
    "\"5\" : \"sadness\",\n",
    "\"6\" : \"surprise\"\n",
    "}\n",
    "def map_ekman_emotion(emo_ds):\n",
    "    final_result_emo = []\n",
    "    for idx, row in emo_ds.iterrows():\n",
    "        e = row[\"emotion_label\"]\n",
    "        l = e[random.randint(0,len(e)-1)]\n",
    "        final_result_emo.append(ekman_mapping[str(l)])\n",
    "    emo_ds[\"emotion_label\"] = final_result_emo\n",
    "    return emo_ds           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/train.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "test_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/test.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "val_ekman = pd.read_csv(\"dataset/GoEmotions-pytorch/data/ekman/dev.tsv\",sep='\\t',names=['utterance', 'emotion_label', 'rater_id'])\n",
    "\n",
    "train_ekman = map_ekman_emotion(format_go_emo(train_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "test_ekman = map_ekman_emotion(format_go_emo(test_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})\n",
    "val_ekman = map_ekman_emotion(format_go_emo(val_ekman)).rename(columns={'utterance':'sentence','emotion_label':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/train_ekman_goemo.csv\")\n",
    "test_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/test_ekman_goemo.csv\")\n",
    "val_ekman.to_csv(\"dataset/GoEmotions-pytorch/dataset_csv/val_ekman_goemo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train_ekman],['goemotion'])\n",
    "val_final = map_and_concat(val_final,[val_ekman],['goemotion'])\n",
    "test_final = map_and_concat(test_final,[test_ekman],['goemotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmoryNLP (BM)\n",
    "[link] https://github.com/emorynlp/emotion-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emonlp_train = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-trn.json\" }, field=\"episodes\")\n",
    "emonlp_val = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-dev.json\" }, field=\"episodes\")\n",
    "emonlp_test = load_dataset(\"json\", data_files={\"train\": \"https://raw.githubusercontent.com/emorynlp/emotion-detection/master/json/emotion-detection-tst.json\" }, field=\"episodes\")\n",
    "\n",
    "\n",
    "def format_emonlp(emonlp_dataset):\n",
    "    utterances_emonlp = []\n",
    "    emotions_emonlp = []\n",
    "    for row in emonlp_dataset['train']:\n",
    "        scenes = row['scenes']\n",
    "        for scene in scenes:\n",
    "            utterances = scene['utterances']\n",
    "            for utterance in utterances:\n",
    "                # extract only text and emotion labels\n",
    "                utterances_emonlp.append(utterance['transcript'])\n",
    "                emotions_emonlp.append(relabel_emotion(utterance['emotion']))\n",
    "    clean_emonlp = {}\n",
    "    clean_emonlp[\"sentence\"] = utterances_emonlp\n",
    "    clean_emonlp[\"label\"] = emotions_emonlp\n",
    "    clean_emonlp = Dataset.from_dict(clean_emonlp).filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_emonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emonlp = format_emonlp(emonlp_train).to_pandas()\n",
    "val_emonlp = format_emonlp(emonlp_val).to_pandas()\n",
    "test_emonlp = format_emonlp(emonlp_test).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emonlp.to_csv('dataset/Emorynlp/emonlp_train_clean.csv', index = False)\n",
    "val_emonlp.to_csv('dataset/Emorynlp/emonlp_val_clean.csv', index = False)\n",
    "test_emonlp.to_csv('dataset/Emorynlp/emonlp_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train_emonlp],[\"emoryNLP\"])\n",
    "val_final = map_and_concat(val_final,[val_emonlp],[\"emoryNLP\"])\n",
    "test_final = map_and_concat(test_final,[test_emonlp],[\"emoryNLP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empathetic Dialogues (excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DailyDialog (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = load_dataset(\"daily_dialog\")\n",
    "\n",
    "def format_dd(daily):\n",
    "    utterances_daily = []\n",
    "    for d in daily[\"dialog\"]:\n",
    "        utterances_daily = utterances_daily + d\n",
    "    emotions_daily = []\n",
    "    for d in daily[\"emotion\"]:\n",
    "        emotions_daily = emotions_daily + d\n",
    "    clean_daily = {}\n",
    "    clean_daily[\"sentence\"] = utterances_daily\n",
    "    clean_daily[\"label\"] = []\n",
    "    for emo in emotions_daily : clean_daily[\"label\"].append(relabel_emotion(emo))\n",
    "    clean_daily = Dataset.from_dict(clean_daily)\n",
    "    clean_daily = clean_daily.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_daily\n",
    "\n",
    "train_dd = format_dd(daily['train']).to_pandas()\n",
    "val_dd = format_dd(daily['validation']).to_pandas()\n",
    "test_dd = format_dd(daily['test']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train_dd],[\"daily\"])\n",
    "val_final = map_and_concat(val_final,[val_dd],[\"daily\"])\n",
    "test_final = map_and_concat(test_final,[test_dd],[\"daily\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEMOCAP (BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_utterance = []\n",
    "def split_string_u(string):\n",
    "    pattern = r'^(.*?)(\\[[\\d.-]+\\]:\\s*)(.*)$'\n",
    "    match = re.match(pattern, string)\n",
    "    \n",
    "    if match:\n",
    "        part1 = match.group(1).rstrip()  # Remove trailing spaces from the first part\n",
    "        part2 = match.group(2).rstrip().rstrip(\":\")  # Remove trailing spaces from the second part\n",
    "        part3 = match.group(3)\n",
    "        return part1, part2, part3\n",
    "    else:\n",
    "        excluded_utterance.append(string)\n",
    "\n",
    "def split_string_e(string):\n",
    "   return string.strip().split('\\t')\n",
    "def read_transcription(path):\n",
    "    transcript_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    utterance_list = []\n",
    "    for u in transcript_ds['test']['text']:\n",
    "        utterance = split_string_u(u)\n",
    "        utterance_list.append(utterance)\n",
    "    return utterance_list\n",
    "\n",
    "def read_emotion(path):\n",
    "    emo_ds = load_dataset(\"text\", data_files={\"test\":path})\n",
    "    print(path + '\\n')\n",
    "    emotion_list = []\n",
    "    for i,e in enumerate(emo_ds['test']['text']):\n",
    "        if (e.strip() == '') and (i + 1 < len(emo_ds['test']['text'])):\n",
    "            emotion = split_string_e(emo_ds['test']['text'][i+1])\n",
    "            emotion_list.append(emotion)        \n",
    "    return emotion_list\n",
    "\n",
    "def read_iemocap(dir_path):\n",
    "    # initial two result df to store all transcriptions from all sessions\n",
    "    # similarly for emotions\n",
    "    transcriptions_df = pd.DataFrame(columns=['idx', 'labs', 'utterance', 'session'])\n",
    "    emotions_df = pd.DataFrame(columns=['labs', 'idx', 'emotion', 'attribute', 'session'])\n",
    "    sessions = ['Session1','Session2','Session3','Session4', 'Session5']\n",
    "    for session in sessions:\n",
    "        # get all transcriptions\n",
    "        transcriptions = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/transcriptions'\n",
    "        all_utterances = []\n",
    "        for item in os.listdir(transcriptions):\n",
    "            item_path = os.path.join(transcriptions, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_utterances = all_utterances + read_transcription(item_path)\n",
    "        u_df = pd.DataFrame(all_utterances, columns=['idx', 'labs', 'utterance'])\n",
    "        u_df['session'] = session\n",
    "        transcriptions_df = pd.concat([transcriptions_df, u_df])\n",
    "        \n",
    "        # get all emotions\n",
    "        emoeval = dir_path + '/IEMOCAP_full_release/' + session + '/' + 'dialog' + '/EmoEvaluation'\n",
    "        all_emotions = []\n",
    "        for item in os.listdir(emoeval):\n",
    "            item_path = os.path.join(emoeval, item)\n",
    "            if os.path.isfile(item_path):\n",
    "                all_emotions = all_emotions + read_emotion(item_path)\n",
    "        e_df = pd.DataFrame(all_emotions, columns=['labs', 'idx', 'emotion', 'attribute']) \n",
    "        e_df['session'] = session\n",
    "        emotions_df = pd.concat([emotions_df, e_df])\n",
    "\n",
    "    return {'utterance':transcriptions_df, \n",
    "            'emotion': emotions_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iemocap_dir = 'dataset/IEMOCAP_full_release_withoutVideos'\n",
    "# join by idx\n",
    "read_results = read_iemocap(iemocap_dir)\n",
    "clean_imocap = pd.merge(read_results['utterance'],read_results['emotion'], on='idx')\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'xxx']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'dis']\n",
    "clean_imocap = clean_imocap[clean_imocap['emotion'] != 'oth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test val split: stratify base on labels\n",
    "train, testval= train_test_split(clean_imocap, test_size=0.20, random_state=0, stratify=clean_imocap[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n",
    "\n",
    "train.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_train.csv')\n",
    "test.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_test.csv')\n",
    "val.rename(columns={'emotion':'label', 'utterance':'sentence'}).to_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_train.csv')\n",
    "test = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_test.csv')\n",
    "val = pd.read_csv('dataset/IEMOCAP_full_release_withoutVideos/IEMOCAP_full_release/iemocap_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[train],[\"iemocap\"])\n",
    "val_final = map_and_concat(val_final,[val],[\"iemocap\"])\n",
    "test_final = map_and_concat(test_final,[test],[\"iemocap\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MELD (BM) (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld = load_dataset(\"csv\", data_files={\"test\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/test_sent_emo.csv\",\n",
    "                                       \"train\":\"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/train_sent_emo.csv\",\n",
    "                                       \"val\": \"https://raw.githubusercontent.com/declare-lab/MELD/master/data/MELD/dev_sent_emo.csv\"})\n",
    "\n",
    "def format_meld(meld):\n",
    "    clean_meld = {}\n",
    "    clean_meld[\"sentence\"] = meld[\"Utterance\"]\n",
    "\n",
    "    clean_meld[\"label\"] = []\n",
    "    for emo in meld[\"Emotion\"] : clean_meld[\"label\"].append(relabel_emotion(emo))\n",
    "    clean_meld = Dataset.from_dict(clean_meld)\n",
    "    clean_meld = clean_meld.filter(lambda e: all(e[field] is not None for field in e))\n",
    "    return clean_meld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train = format_meld(meld['train']).to_pandas()\n",
    "meld_val = format_meld(meld['val']).to_pandas()\n",
    "meld_test = format_meld(meld['test']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meld_train.to_csv('dataset/MELD/meld_train_clean.csv', index = False)\n",
    "meld_val.to_csv('dataset/MELD/meld_val_clean.csv', index = False)\n",
    "meld_test.to_csv('dataset/MELD/meld_test_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemEval-2018, EI-reg, Mohammad et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_semeval(filename, dataset):\n",
    "    new_df = pd.DataFrame(columns=['ID', 'utterance', 'emotion'],dtype=object)\n",
    "    for index, row in dataset.iterrows():\n",
    "        utterance = row['Tweet']\n",
    "        emotions_col = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "        id = index\n",
    "        emotions_list = []\n",
    "        for emotion in emotions_col:\n",
    "            if row[emotion] == 1:\n",
    "                emotions_list.append(emotion) \n",
    "        \n",
    "        if emotions_list: \n",
    "            e = emotions_list[random.randint(0,len(emotions_list)-1)]\n",
    "            new_row = {\"ID\": id,\n",
    "                    \"utterance\": utterance,\n",
    "                    \"emotion\": e}\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    new_df.to_csv(\"dataset/SemEval-2018/\" + filename, index=False)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('sem_eval_2018_task_1', 'subtask5.english')\n",
    "prep_semeval('semeval_train_clean.csv',df['train'].to_pandas())\n",
    "prep_semeval('semeval_test_clean.csv',df['test'].to_pandas())\n",
    "prep_semeval('semeval_val_clean.csv', df['validation'].to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semeval_train = pd.read_csv(\"dataset/SemEval-2018/semeval_train_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_test = pd.read_csv(\"dataset/SemEval-2018/semeval_test_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "semeval_val = pd.read_csv(\"dataset/SemEval-2018/semeval_val_clean.csv\").rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISEAR, Vikash (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear = pd.read_csv('dataset/ISEAR/isear_clean.csv', index_col=0)\n",
    "train, testval= train_test_split(isear, test_size=0.20, random_state=0, stratify=isear[['emotion']])\n",
    "test, val = train_test_split(testval, test_size=0.5, random_state=0, stratify=testval[['emotion']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train = train.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_test = test.rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_val = val.rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_train.csv')\n",
    "isear_test.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_test.csv')\n",
    "isear_val.to_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear_train = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_train.csv').rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_test = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_test.csv').rename(columns={'utterance':'sentence','emotion': 'label'})\n",
    "isear_val = pd.read_csv('/home/annie/Desktop/evaluate_models/dataset/ISEAR/isear_val.csv').rename(columns={'utterance':'sentence','emotion': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CARER Emotion Dataset, Elvis et al. (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "class_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "def map_to_label(dataset):\n",
    "    col = []\n",
    "    for l in dataset['label']:\n",
    "        col.append(class_names[l])\n",
    "    return col\n",
    "\n",
    "test_l = map_to_label(dataset['test'])\n",
    "validation_l = map_to_label(dataset['validation'])\n",
    "train_l = map_to_label(dataset['train'])\n",
    "\n",
    "carer_test = pd.DataFrame({\"sentence\": dataset['test']['text'], 'label': test_l, \"emotion_num\": dataset['test']['label']})\n",
    "carer_test.to_csv(\"dataset/Emotion_Elvis/EmoElvis_test_clean.csv\")\n",
    "\n",
    "carer_val = pd.DataFrame({\"sentence\": dataset['validation']['text'], 'label': validation_l, \"emotion_num\": dataset['validation']['label']})\n",
    "carer_val.to_csv(\"dataset/Emotion_Elvis/EmoElvis_validation_clean.csv\")\n",
    "\n",
    "carer_train = pd.DataFrame({\"sentence\": dataset['train']['text'], 'label': train_l, \"emotion_num\": dataset['train']['label']})\n",
    "carer_train.to_csv(\"dataset/Emotion_Elvis/EmoElvis_train_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crowdflower (hartmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/tlkh/text-emotion-classification/master/dataset/original/text_emotion.csv\"\n",
    "flower = load_dataset(\"csv\", data_files=url )\n",
    "X = flower['train']\n",
    "y = flower['train']['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "train_index, test_index = next(sss.split(X, y))\n",
    "\n",
    "train=X[train_index]\n",
    "testval = X[test_index]\n",
    "testval = Dataset.from_dict(testval)\n",
    "\n",
    "X = testval\n",
    "y = testval['sentiment']\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=0)\n",
    "val_index, test_index = next(sss.split(X, y))\n",
    "val=X[val_index]\n",
    "test=X[test_index]\n",
    "\n",
    "flower_train = pd.DataFrame.from_dict(train).rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_test = pd.DataFrame.from_dict(test).rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_val = pd.DataFrame.from_dict(val).rename(columns={\"sentiment\":'label', 'content':'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_train = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/crowdflower_data/crowdflower_trian.csv\").rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_test = pd.read_csv(\"/home/annie/Desktop/evaluate_models/dataset/crowdflower_data/crowdflower_test.csv\").rename(columns={\"sentiment\":'label', 'content':'sentence'})\n",
    "flower_val = pd.read_csv (\"/home/annie/Desktop/evaluate_models/dataset/crowdflower_data/crowdflower_valid.csv\").rename(columns={\"sentiment\":'label', 'content':'sentence'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_train.to_csv(\"dataset/crowdflower_data/crowdflower_trian.csv\")\n",
    "flower_test.to_csv(\"dataset/crowdflower_data/crowdflower_test.csv\")\n",
    "flower_val.to_csv(\"dataset/crowdflower_data/crowdflower_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[flower_train],[\"flower\"])\n",
    "val_final = map_and_concat(val_final,[flower_val],[\"flower\"])\n",
    "test_final = map_and_concat(test_final,[flower_test],[\"flower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = map_and_concat(train_final,[meld_train,semeval_train,isear_train,carer_train],[\"meld\",\"semeval\",\"isear\",\"carer\"])\n",
    "val_final = map_and_concat(val_final,[meld_val,semeval_val,isear_val,carer_val],[\"meld\",\"semeval\",\"isear\",\"carer\"])\n",
    "test_final = map_and_concat(test_final,[meld_test,semeval_test,isear_test,carer_test],[\"meld\",\"semeval\",\"isear\",\"carer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_final.to_json('dataset/train_final_temp.json',orient='records')\n",
    "# test_final.to_json('dataset/test_final_temp.json',orient='records')\n",
    "# val_final.to_json('dataset/val_final_temp.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "val_basic = pd.DataFrame(columns=['sentence','label','ds_name'])\n",
    "test_basic = pd.DataFrame(columns=['sentence','label','ds_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic=sec_map_and_concat_ekman(train_final)\n",
    "val_basic=sec_map_and_concat_ekman(val_final)\n",
    "test_basic=sec_map_and_concat_ekman(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic=train_basic[train_basic['label']!='tbd']\n",
    "val_basic=val_basic[val_basic['label']!='tbd']\n",
    "test_basic=test_basic[test_basic['label']!='tbd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic.to_json(train_b,orient='records')\n",
    "val_basic.to_json(val_b,orient='records')\n",
    "test_basic.to_json(test_b,orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = ['daily', 'emoryNLP',\n",
    "           'iemocap']\n",
    "train_temp = train_basic[train_basic['ds_name'].isin(dataset_name_list)]\n",
    "val_temp = val_basic[val_basic['ds_name'].isin(dataset_name_list)]\n",
    "test_temp = test_basic[test_basic['ds_name'].isin(dataset_name_list)]\n",
    "t_anger = train_temp[train_temp['label'] == 'anger']\n",
    "tst_anger = test_temp[test_temp['label'] == 'anger']\n",
    "v_anger = val_temp[val_temp['label'] == 'anger']\n",
    "\n",
    "train_no_anger = train_basic.drop(index=t_anger.index)\n",
    "val_no_anger = val_basic.drop(index=v_anger.index)\n",
    "test_no_anger = test_basic.drop(index=tst_anger.index)\n",
    "\n",
    "train_no_anger.to_json(train_no_anger_f4, orient=\"records\")\n",
    "val_no_anger.to_json(val_no_anger_f4,orient='records')\n",
    "test_no_anger.to_json(test_no_anger_f4,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CancerEMO (medicine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger = pd.read_csv('dataset/CancerEMO/Anger_anon.csv')\n",
    "anticipation = pd.read_csv('dataset/CancerEMO/Anticipation_anon.csv')\n",
    "disgust = pd.read_csv('dataset/CancerEMO/Disgust_anon.csv')\n",
    "surprise = pd.read_csv('dataset/CancerEMO/Surprise_anon.csv')\n",
    "sadness = pd.read_csv('dataset/CancerEMO/Sadness_anon.csv')\n",
    "joy = pd.read_csv('dataset/CancerEMO/Joy_anon.csv')\n",
    "trust = pd.read_csv('dataset/CancerEMO/Trust_anon.csv')\n",
    "fear = pd.read_csv('dataset/CancerEMO/Fear_anon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disgust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic = pd.read_json(train_b_og)\n",
    "val_basic = pd.read_json(val_b_og)\n",
    "test_basic = pd.read_json(test_b_og)\n",
    "all_basic = pd.concat([train_basic,val_basic,test_basic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anger = pd.read_json(train_no_anger_f4_og)\n",
    "val_no_anger = pd.read_json(val_no_anger_f4_og)\n",
    "test_no_anger = pd.read_json(test_no_anger_f4_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You know that is tempting but is really not g...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you mean ? It will help us to relax .</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you really think so ? I don't . It will ju...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I guess you are right.But what shall we do ? ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181417</th>\n",
       "      <td>@Ms_Asia_Pacific @jgsuing @PilosopoTanya @Huff...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181418</th>\n",
       "      <td>.@Travelanswerman: The possibilities R endless...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181419</th>\n",
       "      <td>You have a #problem? Yes! Can you do #somethin...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181420</th>\n",
       "      <td>@andreamitchell said @berniesanders not only d...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181421</th>\n",
       "      <td>i wonder how a guy can broke his penis while h...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence     label\n",
       "0       Say , Jim , how about going for a few beers af...   neutral\n",
       "1        You know that is tempting but is really not g...   neutral\n",
       "2          What do you mean ? It will help us to relax .    neutral\n",
       "3        Do you really think so ? I don't . It will ju...   neutral\n",
       "4        I guess you are right.But what shall we do ? ...   neutral\n",
       "...                                                   ...       ...\n",
       "181417  @Ms_Asia_Pacific @jgsuing @PilosopoTanya @Huff...     anger\n",
       "181418  .@Travelanswerman: The possibilities R endless...       joy\n",
       "181419  You have a #problem? Yes! Can you do #somethin...   disgust\n",
       "181420  @andreamitchell said @berniesanders not only d...  surprise\n",
       "181421  i wonder how a guy can broke his penis while h...  surprise\n",
       "\n",
       "[181422 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_anger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = count_labels(train_no_anger)\n",
    "v = count_labels(val_no_anger)\n",
    "tst = count_labels(test_no_anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_summary = pd.DataFrame({\n",
    "    \"label\":tr['label'],\n",
    "    'tr': tr['count'],\n",
    "    'val': v['count'],\n",
    "    'tst':tst['count']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_summary.to_csv('dataset/labels_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(train_b)\n",
    "val = pd.read_json(val_b)\n",
    "test = pd.read_json(test_b)\n",
    "\n",
    "train_agg = count_labels(train)\n",
    "train_avg = round(train_agg['count'].mean())\n",
    "test_agg = count_labels(test)\n",
    "test_avg = round(test_agg['count'].mean())\n",
    "val_agg = count_labels(val)\n",
    "val_avg = round(val_agg['count'].mean())\n",
    "\n",
    "\n",
    "list = [(train, train_agg, train_avg),(test, test_agg, test_avg), (val, val_agg, val_avg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over+UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_basic_sampling(df, df_agg, avg):\n",
    "    result_df = pd.DataFrame()\n",
    "    undersampled_dfs = df_agg[df_agg['count'] >= avg]['label'] \n",
    "    for ud_df in undersampled_dfs:\n",
    "        u =  df[df['label'] == ud_df].sample(avg)\n",
    "        result_df = pd.concat([result_df,u])\n",
    "        \n",
    "    oversampled_dfs = df_agg[df_agg['count'] < avg]['label'] \n",
    "    for ov_df in oversampled_dfs:\n",
    "        o =  df[df['label'] == ov_df].sample(avg, replace=True)\n",
    "        result_df = pd.concat([result_df,o])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_basic_sampling(list[0][0],list[0][1],list[0][2]).to_json(train_b_sample, orient='records')\n",
    "do_basic_sampling(list[1][0],list[1][1],list[1][2]).to_json(test_b_sample, orient='records')\n",
    "do_basic_sampling(list[2][0],list[2][1],list[2][2]).to_json(val_b_sample, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT Diverse Beam Search for Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams,\n",
    "    num_return_sequences,\n",
    "    num_beam_groups = 2,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generative_sampling(df, df_agg, avg):\n",
    "    result_df = pd.DataFrame()\n",
    "    undersampled_dfs = df_agg[df_agg['count'] >= avg]['label'] \n",
    "    for ud_df in undersampled_dfs:\n",
    "        u =  df[df['label'] == ud_df].sample(avg)\n",
    "        result_df = pd.concat([result_df,u])\n",
    "        \n",
    "    oversampled_dfs = df_agg[df_agg['count'] < avg]['label'] \n",
    "    for ov_df in oversampled_dfs:\n",
    "        label_count = df_agg[df_agg['label'] == ov_df]['count']\n",
    "        difference = math.ceil(avg-label_count)\n",
    "        num_candidates_per_sent = math.ceil((difference)/label_count)\n",
    "        num_beams = num_candidates_per_sent * 2\n",
    "        num_return_sequences = num_candidates_per_sent\n",
    "        print(ov_df)\n",
    "        print('--------- num_candidates_per_sent --------\\n')\n",
    "        print(num_candidates_per_sent)   \n",
    "        sentences = df[df['label'] == ov_df]['sentence']\n",
    "        sentences_to_paraphrase = sentences\n",
    "        if num_candidates_per_sent == 1:\n",
    "            sentences_to_paraphrase = sentences.sample(difference)\n",
    "        result_generated_sentences = []\n",
    "        for sent in sentences_to_paraphrase: \n",
    "            sent_paraphrase_candidates = paraphrase(sent,num_beams,num_return_sequences)\n",
    "            sent_paraphrases_list = sent_paraphrase_candidates[:num_candidates_per_sent]\n",
    "            result_generated_sentences = result_generated_sentences + sent_paraphrases_list\n",
    "        result_sentences = sentences.to_list() + result_generated_sentences\n",
    "        result_dict = dict(sentence=result_sentences, label=ov_df)\n",
    "        o = pd.DataFrame.from_dict(result_dict)\n",
    "        result_df = pd.concat([result_df,o])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_sampling(list[0][0],list[0][1],list[0][2]).to_json(test_b_beam, orient='records')\n",
    "generative_sampling(list[1][0],list[1][1],list[1][2]).to_json(test_b_beam, orient='records')\n",
    "generative_sampling(list[2][0],list[2][1],list[2][2]).to_json(val_b_beam, orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! Additional step to prepare data for training!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.read_json(\"/home/annie/Desktop/EmoRec/data/dataset/train_no_anger_f4.json\")\n",
    "tst = pd.read_json(\"/home/annie/Desktop/EmoRec/data/dataset/test_no_anger_f4.json\")\n",
    "vv = pd.read_json(\"/home/annie/Desktop/EmoRec/data/dataset/val_no_anger_f4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.drop(columns=['ds_name']).to_json(\"/home/annie/Desktop/EmoRec/data/dataset/train_no_anger_f4_train.json\", orient='records')\n",
    "tst.drop(columns=['ds_name']).to_json(\"/home/annie/Desktop/EmoRec/data/dataset/test_no_anger_f4_test.json\", orient='records')\n",
    "vv.drop(columns=['ds_name']).to_json(\"/home/annie/Desktop/EmoRec/data/dataset/val_no_anger_f4_val.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_json('/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/train_basic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.read_json(\"/home/annie/Desktop/EmoRec/data/dataset/train_basic_train.json\")\n",
    "to = pd.read_json(\"/home/annie/Desktop/haru-nlp-train-fork/src/haru_nlp_train/text-classification/dataset/train_basic.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(tt['sentence']) & set(to['sentence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.merge(to, how='inner', on=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anger = pd.read_json(train_no_anger_f4)\n",
    "val_no_anger = pd.read_json(val_no_anger_f4)\n",
    "test_no_anger = pd.read_json(test_no_anger_f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily</td>\n",
       "      <td>86343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goemotion</td>\n",
       "      <td>43410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flower</td>\n",
       "      <td>30117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carer</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meld</td>\n",
       "      <td>9989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emoryNLP</td>\n",
       "      <td>8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>isear</td>\n",
       "      <td>6132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>semeval</td>\n",
       "      <td>6124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iemocap</td>\n",
       "      <td>3660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ds_name  count\n",
       "0      daily  86343\n",
       "1  goemotion  43410\n",
       "2     flower  30117\n",
       "3      carer  16000\n",
       "4       meld   9989\n",
       "5   emoryNLP   8074\n",
       "6      isear   6132\n",
       "7    semeval   6124\n",
       "8    iemocap   3660"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_anger['ds_name'].value_counts().reset_index(name = 'count').rename(columns={'index':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily</td>\n",
       "      <td>7992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goemotion</td>\n",
       "      <td>5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flower</td>\n",
       "      <td>3766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carer</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meld</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emoryNLP</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semeval</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>isear</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iemocap</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ds_name  count\n",
       "0      daily   7992\n",
       "1  goemotion   5426\n",
       "2     flower   3766\n",
       "3      carer   2000\n",
       "4       meld   1109\n",
       "5   emoryNLP   1067\n",
       "6    semeval    804\n",
       "7      isear    767\n",
       "8    iemocap    457"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_no_anger['ds_name'].value_counts().reset_index(name = 'count').rename(columns={'index':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily</td>\n",
       "      <td>7622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goemotion</td>\n",
       "      <td>5427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flower</td>\n",
       "      <td>3764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semeval</td>\n",
       "      <td>2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meld</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>carer</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emoryNLP</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>isear</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iemocap</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ds_name  count\n",
       "0      daily   7622\n",
       "1  goemotion   5427\n",
       "2     flower   3764\n",
       "3    semeval   2966\n",
       "4       meld   2610\n",
       "5      carer   2000\n",
       "6   emoryNLP   1070\n",
       "7      isear    767\n",
       "8    iemocap    458"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_no_anger['ds_name'].value_counts().reset_index(name = 'count').rename(columns={'index':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
